{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230070, 32, 32, 1)\n",
      "(230070, 6)\n",
      "(5684, 32, 32, 1)\n",
      "(5684, 6)\n",
      "(13068, 32, 32, 1)\n",
      "(13068, 6)\n"
     ]
    }
   ],
   "source": [
    "file = open('SVHN_multi.pickle','rb')\n",
    "data=pickle.load(file)\n",
    "train_data=data['train_dataset']\n",
    "train_label=data['train_labels']\n",
    "valid_data=data['valid_dataset']\n",
    "valid_label=data['valid_labels']\n",
    "test_data=data['test_dataset']\n",
    "test_label=data['test_labels']\n",
    "\n",
    "\n",
    "print (train_data.shape)\n",
    "print (train_label.shape)\n",
    "print (valid_data.shape)\n",
    "print (valid_label.shape)\n",
    "print (test_data.shape)\n",
    "print (test_label.shape)\n",
    "\n",
    "#print train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias shape\n",
      "(16,)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(64, 28, 28, 16)\n",
      "add bias\n",
      "(64, 28, 28, 16)\n",
      "Relu\n",
      "(64, 28, 28, 16)\n",
      "max pool\n",
      "(64, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(64, 1, 1, 128)\n",
      "[64, 128]\n",
      "reshape shape\n",
      "(64, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(64, 28, 28, 16)\n",
      "add bias\n",
      "(64, 28, 28, 16)\n",
      "Relu\n",
      "(64, 28, 28, 16)\n",
      "max pool\n",
      "(64, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(64, 1, 1, 128)\n",
      "[64, 128]\n",
      "reshape shape\n",
      "(64, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(64, 28, 28, 16)\n",
      "add bias\n",
      "(64, 28, 28, 16)\n",
      "Relu\n",
      "(64, 28, 28, 16)\n",
      "max pool\n",
      "(64, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(64, 1, 1, 128)\n",
      "[64, 128]\n",
      "reshape shape\n",
      "(64, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(64, 28, 28, 16)\n",
      "add bias\n",
      "(64, 28, 28, 16)\n",
      "Relu\n",
      "(64, 28, 28, 16)\n",
      "max pool\n",
      "(64, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(64, 1, 1, 128)\n",
      "[64, 128]\n",
      "reshape shape\n",
      "(64, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(64, 28, 28, 16)\n",
      "add bias\n",
      "(64, 28, 28, 16)\n",
      "Relu\n",
      "(64, 28, 28, 16)\n",
      "max pool\n",
      "(64, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(64, 1, 1, 128)\n",
      "[64, 128]\n",
      "reshape shape\n",
      "(64, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(64, 28, 28, 16)\n",
      "add bias\n",
      "(64, 28, 28, 16)\n",
      "Relu\n",
      "(64, 28, 28, 16)\n",
      "max pool\n",
      "(64, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(64, 1, 1, 128)\n",
      "[64, 128]\n",
      "reshape shape\n",
      "(64, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(5684, 28, 28, 16)\n",
      "add bias\n",
      "(5684, 28, 28, 16)\n",
      "Relu\n",
      "(5684, 28, 28, 16)\n",
      "max pool\n",
      "(5684, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(5684, 1, 1, 128)\n",
      "[5684, 128]\n",
      "reshape shape\n",
      "(5684, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(5684, 28, 28, 16)\n",
      "add bias\n",
      "(5684, 28, 28, 16)\n",
      "Relu\n",
      "(5684, 28, 28, 16)\n",
      "max pool\n",
      "(5684, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(5684, 1, 1, 128)\n",
      "[5684, 128]\n",
      "reshape shape\n",
      "(5684, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(5684, 28, 28, 16)\n",
      "add bias\n",
      "(5684, 28, 28, 16)\n",
      "Relu\n",
      "(5684, 28, 28, 16)\n",
      "max pool\n",
      "(5684, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(5684, 1, 1, 128)\n",
      "[5684, 128]\n",
      "reshape shape\n",
      "(5684, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(5684, 28, 28, 16)\n",
      "add bias\n",
      "(5684, 28, 28, 16)\n",
      "Relu\n",
      "(5684, 28, 28, 16)\n",
      "max pool\n",
      "(5684, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(5684, 1, 1, 128)\n",
      "[5684, 128]\n",
      "reshape shape\n",
      "(5684, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(5684, 28, 28, 16)\n",
      "add bias\n",
      "(5684, 28, 28, 16)\n",
      "Relu\n",
      "(5684, 28, 28, 16)\n",
      "max pool\n",
      "(5684, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(5684, 1, 1, 128)\n",
      "[5684, 128]\n",
      "reshape shape\n",
      "(5684, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(13068, 28, 28, 16)\n",
      "add bias\n",
      "(13068, 28, 28, 16)\n",
      "Relu\n",
      "(13068, 28, 28, 16)\n",
      "max pool\n",
      "(13068, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(13068, 1, 1, 128)\n",
      "[13068, 128]\n",
      "reshape shape\n",
      "(13068, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(13068, 28, 28, 16)\n",
      "add bias\n",
      "(13068, 28, 28, 16)\n",
      "Relu\n",
      "(13068, 28, 28, 16)\n",
      "max pool\n",
      "(13068, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(13068, 1, 1, 128)\n",
      "[13068, 128]\n",
      "reshape shape\n",
      "(13068, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(13068, 28, 28, 16)\n",
      "add bias\n",
      "(13068, 28, 28, 16)\n",
      "Relu\n",
      "(13068, 28, 28, 16)\n",
      "max pool\n",
      "(13068, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(13068, 1, 1, 128)\n",
      "[13068, 128]\n",
      "reshape shape\n",
      "(13068, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(13068, 28, 28, 16)\n",
      "add bias\n",
      "(13068, 28, 28, 16)\n",
      "Relu\n",
      "(13068, 28, 28, 16)\n",
      "max pool\n",
      "(13068, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(13068, 1, 1, 128)\n",
      "[13068, 128]\n",
      "reshape shape\n",
      "(13068, 128)\n",
      "model\n",
      "[64, 1024]\n",
      "before bias\n",
      "(13068, 28, 28, 16)\n",
      "add bias\n",
      "(13068, 28, 28, 16)\n",
      "Relu\n",
      "(13068, 28, 28, 16)\n",
      "max pool\n",
      "(13068, 14, 14, 16)\n",
      "(5, 5, 16, 32)\n",
      "Layer3 shape\n",
      "(13068, 1, 1, 128)\n",
      "[13068, 128]\n",
      "reshape shape\n",
      "(13068, 128)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 11.989478\n",
      "Minibatch accuracy: 5.9%\n",
      "Validation accuracy: 57.5%\n",
      "Minibatch loss at step 500: 7.062858\n",
      "Minibatch accuracy: 50.9%\n",
      "Validation accuracy: 57.5%\n",
      "Minibatch loss at step 1000: 6.557072\n",
      "Minibatch accuracy: 54.7%\n",
      "Validation accuracy: 57.5%\n",
      "Minibatch loss at step 1500: 6.647480\n",
      "Minibatch accuracy: 55.0%\n",
      "Validation accuracy: 57.4%\n",
      "Test accuracy: 70.5%\n",
      "Model saved in file: CNN_multi1.ckpt\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 2).T == labels) / predictions.shape[1] / predictions.shape[0])\n",
    "#CNN\n",
    "#Weight and Bias initialization\n",
    "pixel =32\n",
    "labels=11\n",
    "channels=1\n",
    "noofimages = 64\n",
    "stride=5\n",
    "noffilters1=16\n",
    "noffilters2=32\n",
    "hidden=128\n",
    "\n",
    "shape = [noofimages, pixel, pixel, channels]\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "        #tf.get_variable --shared variable\n",
    "        tf_train_data = tf.placeholder(tf.float32, shape=(noofimages, pixel, pixel, channels))\n",
    "        tf_train_label = tf.placeholder(tf.int32, shape=(noofimages, 6))\n",
    "        tf_valid_data = tf.constant(valid_data)\n",
    "        tf_test_data = tf.constant(test_data)\n",
    "        #weight initialization method --xavier\n",
    "        w1=tf.get_variable(\"weight1\", shape=[stride, stride, channels, noffilters1],\\\n",
    "           initializer=tf.constant_initializer(0))\n",
    "        #b1=tf.get_variable(\"bias1\",initializer=tf.zeros([noffilters1]))\n",
    "        b1=tf.Variable(tf.constant(1.0, shape=[noffilters1]), name='B1')\n",
    "        print (\"bias shape\")\n",
    "        print (b1.get_shape())\n",
    "        w2=tf.get_variable(\"weight2\", shape=[stride, stride, noffilters1, noffilters2],\\\n",
    "          initializer=tf.constant_initializer(0))\n",
    "        b2=tf.Variable(tf.constant(1.0, shape=[noffilters2]), name='B2')\n",
    "        w3=tf.get_variable(\"weight3\", shape=[stride, stride, noffilters2, hidden],\\\n",
    "           initializer=tf.constant_initializer(0))\n",
    "        b3=tf.Variable(tf.constant(1.0, shape=[hidden]), name='B3')\n",
    "        \n",
    "        \n",
    "        nn_w1=tf.get_variable(\"nn_weight1\", shape=[hidden,labels],\\\n",
    "             initializer=tf.constant_initializer(0))\n",
    "        nn_b1=tf.Variable(tf.constant(1.0, shape=[labels]), name='nnbias1')\n",
    "        nn_w2=tf.get_variable(\"nn_weight2\", shape=[hidden,labels],\\\n",
    "             initializer=tf.constant_initializer(0))\n",
    "        nn_b2=tf.Variable(tf.constant(1.0, shape=[labels]), name='nnbias2')\n",
    "        nn_w3=tf.get_variable(\"nn_weight3\",shape=[hidden,labels],\\\n",
    "             initializer=tf.constant_initializer(0))\n",
    "        nn_b3=tf.Variable(tf.constant(1.0, shape=[labels]), name='nnbias3')\n",
    "        nn_w4=tf.get_variable(\"nn_weight4\",shape=[hidden,labels],\\\n",
    "             initializer=tf.constant_initializer(0))\n",
    "        nn_b4=tf.Variable(tf.constant(1.0, shape=[labels]), name='nnbias4')\n",
    "        nn_w5=tf.get_variable(\"nn_weight5\",shape=[hidden,labels],\\\n",
    "             initializer=tf.constant_initializer(0))\n",
    "        nn_b5=tf.Variable(tf.constant(1.0, shape=[labels]), name='nnbias5')\n",
    "        \n",
    "        \n",
    "        def model(data,shape):\n",
    "            print (\"model\")\n",
    "            print ([shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            layer1 = tf.nn.conv2d(data,w1, [1,1,1,1],padding='VALID', name='cnn1')\n",
    "            #add bias\n",
    "            print (\"before bias\")\n",
    "            print (layer1.get_shape())\n",
    "            layer1=layer1+b1\n",
    "            print (\"add bias\")\n",
    "            print (layer1.get_shape())\n",
    "            #normalize\n",
    "            layer1=tf.nn.relu(layer1)\n",
    "            print (\"Relu\")\n",
    "            print (layer1.get_shape())\n",
    "            layer1 = tf.nn.local_response_normalization(layer1)\n",
    "            #max-pool\n",
    "            layer1=tf.nn.max_pool(layer1, [1,2,2,1], [1,2,2,1], 'SAME', name='m1')\n",
    "            \n",
    "            print (\"max pool\")\n",
    "            print (layer1.get_shape())\n",
    "            \n",
    "            print (w2.get_shape())\n",
    "            layer2=tf.nn.conv2d(layer1, w2, [1,1,1,1], padding='VALID', name='cnn2')\n",
    "            #add bias\n",
    "            layer2=layer2+b2\n",
    "            #normalize\n",
    "            layer2=tf.nn.relu(layer2)\n",
    "            layer2 = tf.nn.local_response_normalization(layer2)\n",
    "            #max-pool\n",
    "            layer2=tf.nn.max_pool(layer2, [1,2,2,1], [1,2,2,1], 'SAME', name='m2')\n",
    "            \n",
    "            #print (layer1.get_shape())\n",
    "            layer3=tf.nn.conv2d(layer2, w3, [1,1,1,1], padding='VALID', name='cnn3')\n",
    "            #add bia2\n",
    "            layer3=layer3+b3\n",
    "            #normalize\n",
    "            layer3=tf.nn.relu(layer3)\n",
    "            \n",
    "           \n",
    "            #max-pool\n",
    "            #layer3=tf.nn.max_pool(layer3, [1,2,2,1], [1,2,2,1], 'SAME', name='m3')\n",
    "            \n",
    "            #flatten\n",
    "            layer3 = tf.nn.dropout(layer3, 0.9375)\n",
    "            print (\"Layer3 shape\")\n",
    "            print (layer3.get_shape())\n",
    "            \n",
    "            shape = layer3.get_shape().as_list()\n",
    "            print ([shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            reshape = tf.reshape(layer3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "            print (\"reshape shape\")\n",
    "            print (reshape.get_shape())\n",
    "            logits1 = tf.matmul(reshape, nn_w1) + nn_b1\n",
    "            logits2 = tf.matmul(reshape, nn_w2) + nn_b2\n",
    "            logits3 = tf.matmul(reshape, nn_w3) + nn_b3\n",
    "            logits4 = tf.matmul(reshape, nn_w4) + nn_b4\n",
    "            logits5 = tf.matmul(reshape, nn_w5) + nn_b5\n",
    "            return [logits1, logits2, logits3, logits4, logits5]\n",
    "        \n",
    "        [logits1, logits2, logits3, logits4, logits5] = model(tf_train_data, shape)\n",
    "            #Loss function\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, tf_train_label[:,1])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, tf_train_label[:,2])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, tf_train_label[:,3])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, tf_train_label[:,4])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, tf_train_label[:,5]))\n",
    "\n",
    "      # Optimizer.\n",
    "     #optimizer = tf.train.AdagradOptimizer(0.01).minimize(loss) //static learning rate\n",
    "        global_step = tf.Variable(0)\n",
    "    #learning rate with exponential decay.\n",
    "        learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95) \n",
    "        optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "         # Predictions for the training, validation, and test data.\n",
    "        train_prediction =tf.pack([tf.nn.softmax(model(tf_train_data, shape)[0]),\\\n",
    "                          tf.nn.softmax(model(tf_train_data, shape)[1]),\\\n",
    "                          tf.nn.softmax(model(tf_train_data, shape)[2]),\\\n",
    "                          tf.nn.softmax(model(tf_train_data, shape)[3]),\\\n",
    "                          tf.nn.softmax(model(tf_train_data, shape)[4])])\n",
    "        valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_data, shape)[0]),\\\n",
    "                          tf.nn.softmax(model(tf_valid_data, shape)[1]),\\\n",
    "                          tf.nn.softmax(model(tf_valid_data, shape)[2]),\\\n",
    "                          tf.nn.softmax(model(tf_valid_data, shape)[3]),\\\n",
    "                          tf.nn.softmax(model(tf_valid_data, shape)[4])])\n",
    "        test_prediction = tf.pack([tf.nn.softmax(model(tf_test_data, shape)[0]),\\\n",
    "                         tf.nn.softmax(model(tf_test_data, shape)[1]),\\\n",
    "                         tf.nn.softmax(model(tf_test_data,shape)[2]),\\\n",
    "                         tf.nn.softmax(model(tf_test_data,shape)[3]),\\\n",
    "                         tf.nn.softmax(model(tf_test_data,shape)[4])])\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "num_steps = 2000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()  \n",
    "        print('Initialized')\n",
    "        for step in range(num_steps):\n",
    "                offset = (step * noofimages) % (train_label.shape[0] - noofimages)\n",
    "                batch_data = train_data[offset:(offset + noofimages), :, :, :]\n",
    "                batch_labels = train_label[offset:(offset + noofimages),:]\n",
    "                feed_dict = {tf_train_data : batch_data, tf_train_label : batch_labels}\n",
    "                _, l, predictions = session.run(\n",
    "                  [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "              \n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_label[:,1:6]))\n",
    "        save_path = saver.save(session, \"CNN.ckpt\")\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
